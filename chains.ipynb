{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: A library used to build language model applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.ollama import Ollama\n",
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "from langchain.schema import HumanMessage, StrOutputParser, BaseOutputParser\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.output_parsers import (\n",
    "  PydanticOutputParser, \n",
    "  CommaSeparatedListOutputParser,\n",
    "  DatetimeOutputParser,\n",
    "  EnumOutputParser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, LangChain consists of a `chain` or sequence which contains:\n",
    "1. LLM\n",
    "2. Prompt\n",
    "3. Parser\n",
    "\n",
    "The most basic prompt, with the default parser, can be invoked by the LLM using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "llm(\"What do you think of the color green?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, asking a single quesiton is simple, so if you want to build more advanced LLM solutions, you will need to use LangChain chains. There are two ways to create chains:\n",
    "\n",
    "1. `Chain` interface (considered legacy)\n",
    "2. `LCEL` pipelines\n",
    "\n",
    "Here is a basic legacy chain, with the introduction of a prompt that will be chained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"{question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(question=\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same chain but using LCEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke({\"question\": \"What is the meaning of life?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM\n",
    "\n",
    "There are two types of language models:\n",
    "* `LLM`: a model that takes a string as input and returns a string\n",
    "* `ChatModel`: a model that takes a list of messages as input and returns a message\n",
    "\n",
    "The basic `LLM` is often referred to as an `Instruct` model, whereas the other is referred to as a `Chat` model. Ultimately, these are both foundational LLM models fine-tuned for instruction and conversations.\n",
    "\n",
    "We already saw the basic usage of a LLM. Here is a simple example of a ChatModel that uses messages, where the `HumanMessage` is passed in and it returns an `AIMessage`. All messages are derived from the `BaseMessage` which has a `role` and `content`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "input_message = HumanMessage(content=\"how many days are in a year?\")\n",
    "llm([input_message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also not limited to online LLMs. Here is an example using `Ollama` with the `Mistral` LLM running local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral\")\n",
    "print(llm(\"The first man on the moon was ...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also stream the LLM response instead of waiting for the entire text to be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "  model=\"mistral\",\n",
    "  callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "llm(\"Who is Elon Musk?\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also worth noting, it is easy to use models on HuggingFace using their `huggingface_hub` library, just make sure your `HUGGINGFACEHUB_API_TOKEN` is setup in the environment. It can also be slow, since you are running on the shared infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(\n",
    "  repo_id=\"google/flan-t5-xl\", \n",
    "  model_kwargs={\"temperature\": 1}\n",
    ")\n",
    "llm(\"translate English to German: Hello, my name is John.\", raw_response=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Prompts are the instructions to the LLM. There are two tools provided by LangChain for prompts:\n",
    "1. `Prompt Templates`: parameterized prompts\n",
    "2. `Example Selectors`: dynamically select examples to include in the prompts\n",
    "\n",
    "Up to this point, the prompts have been simple strings. However, usually the prompts will be more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"What is a good company that makes {product}?\")\n",
    "print(prompt.format(product=\"cars\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PromptTemplate` works with basic strings but you can also use the more powerful `ChatPromptTemplate` which works with messages and `Chat` models. The types of possible messages are:\n",
    "\n",
    "1. System\n",
    "2. Human\n",
    "3. AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are able to translate from {in_language} to {out_language}.\"),\n",
    "  (\"human\", \"{text}\")\n",
    "])\n",
    "print(prompt.format(in_language=\"English\", out_language=\"German\", text=\"Hello, my name is John.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is just a shortcut way of using special messages, which can be non-variable messages or message prompt templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  SystemMessagePromptTemplate(prompt=PromptTemplate.from_template(\"You are able to translate from {in_language} to {out_language}.\")),\n",
    "  HumanMessage(content=\"USER:\"),\n",
    "  HumanMessagePromptTemplate(prompt=PromptTemplate.from_template(\"{text}\")),\n",
    "])\n",
    "print(prompt.format(in_language=\"English\", out_language=\"German\", text=\"Hello, my name is John.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt templates also implement the `Runnable` interface, which is how they can be used with LCEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"My name is {name}?\")\n",
    "prompt.invoke({\"name\": \"John\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([('human', 'My name is {name}?')])\n",
    "prompt.invoke({\"name\": \"John\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also very common to include a few examples within a prompt, referred to as `one-shot` or `few-shot` examples. The most basic way of doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "  {\n",
    "    \"question\": \"is the name Brian a cool name?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "The length of the name Brian is 5 characters.\n",
    "Because the name has an odd length, it is NOT a cool name.\n",
    "\"\"\"\n",
    "  },  \n",
    "  {\n",
    "    \"question\": \"is the name Tami a cool name?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "The length of the name Tami is 4 characters.\n",
    "Because the name has an even length, it is a cool name.\n",
    "\"\"\"\n",
    "  },  \n",
    "  {\n",
    "    \"question\": \"is the name Jason a cool name?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "The length of the name Jason is 5 characters.\n",
    "Because the name has an odd length, it is NOT a cool name.\n",
    "\"\"\"\n",
    "  },  \n",
    "  {\n",
    "    \"question\": \"is the name Nick a cool name?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "The length of the name Nick is 4 characters.\n",
    "Because the name has an even length, it is a cool name.\n",
    "\"\"\"\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "  examples=examples,\n",
    "  example_prompt=example_prompt,\n",
    "  suffix=\"Question: {input}\",\n",
    "  input_variables=[\"input\"]\n",
    ")\n",
    "print(prompt.format(input=\"is the name Jack a cool name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works fine if you want to include all the examples in every prompt. However, if you want to only select some of the examples, then you need to use an `ExampleSelector`. In this case, we will use the `SemanticSimilarityExampleSelector` which will decide which examples to include based off of similarity of the input and the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "  examples, # examples to  select from\n",
    "  OpenAIEmbeddings(), # used to create the embeddings\n",
    "  Chroma, # VectorStore class\n",
    "  k=1 # number of examples to produce; one-shot in this case\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can select the examples based off a new question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"is the name Jack a cool name?\"\n",
    "example_selector.select_examples({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the example selector we can now define a `FewShotPromptTemplate` without passing in all the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=example_prompt,\n",
    "  suffix=\"Question: {input}\",\n",
    "  input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Is Brian a cool name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the prompt template defined, let's use it with a LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral\")\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"input\": \"is the name Nick a cool name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using examples with a `Chat` is slightly different but not too much.\n",
    "\n",
    "Here is the simplest example, where `FewShotChatMessagePromptTemplate` is included in every message. This example will also demonstrate that the example few shot prompt template doesn't need to include the `input` suffix, but can be included within another prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "  {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "  {\"input\": \"2+3\", \"output\": \"5\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"human\", \"{input}\"),\n",
    "  (\"ai\", \"{output}\")\n",
    "])\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "  example_prompt=example_prompt,\n",
    "  examples=examples\n",
    ")\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a wizard of math.\"),\n",
    "  few_shot_prompt,\n",
    "  (\"human\", \"{input}\")\n",
    "])\n",
    "print(chat_prompt.format(input=\"5+2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look at dynamic examples, which is something we've already seen. However, I will also show working with a `VectorStore` to create an examples selector using mixed examples as would be seen in a chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "    {\"input\": \"2+4\", \"output\": \"6\"},\n",
    "    {\"input\": \"Who are you?\", \"output\": \"My name is Mistral.\"},\n",
    "    {\"input\": \"Hello\", \"output\": \"Hello, my name is Mistral.\"}\n",
    "]\n",
    "to_vectorize = [\" \".join(e.values()) for e in examples]\n",
    "print(to_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "  vectorstore=vectorstore,\n",
    "  k=2\n",
    ")\n",
    "example_selector.select_examples({\"input\": \"2+2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the few shot prompt template using the example selector and example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "  input_variables=[\"input\"],\n",
    "  example_selector=example_selector,\n",
    "  example_prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "  ])\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format(input=\"2+7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a wizard of math.\"),\n",
    "  few_shot_prompt,\n",
    "  (\"human\", \"{input}\")\n",
    "])\n",
    "print(final_prompt.format(input=\"5+2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use the prompt with a `Chat` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "chain = final_prompt | llm\n",
    "chain.invoke({\"input\": \"5+2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output parsers convert the raw output from the language model into a format that you want to use. Most models will return a `string` and the default and most basic parser is the `StrOutputParser`. All parsers are based on the `BaseOutputParser` interface and have a `parse()` function. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StrOutputParser().parse(\"my output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have output that is structured, like with Json, then you need to define the data structure with `pydantic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "  setup: str = Field(description=\"set up for the joke\")\n",
    "  punchline: str = Field(description=\"punchline for the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "parser.parse('{\"setup\": \"What do you call a bear with no teeth?\", \"punchline\": \"A gummy bear!\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see it all together in a chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You create jokes with a <setup> and a <punchline> about the topic provided by the user. Return the joke as JSON with a <setup> and <punchline> property.\"),\n",
    "  (\"human\", \"{input}\")\n",
    "])\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"Tell me a joke about dentists.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of built-in parsers like the two we have already seen. Here are few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CommaSeparatedListOutputParser().parse(\"1, 2, 3, 4, 5\") # the space between is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatetimeOutputParser().parse(\"2008-01-03T18:15:05.000000Z\") # ISO 8601 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors(Enum):\n",
    "  RED = \"red\"\n",
    "  BLUE = \"blue\"\n",
    "  GREEN = \"green\"\n",
    "\n",
    "EnumOutputParser(enum=Colors).parse(\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how easy it is to create your own output parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterCommaSeperatedListOutputParser(BaseOutputParser):\n",
    "  def parse(self, text: str) -> list:\n",
    "    return text.strip().split(\",\")\n",
    "\n",
    "BetterCommaSeperatedListOutputParser().parse(\"1,2,3,4,5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
