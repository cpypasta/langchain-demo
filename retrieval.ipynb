{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is the process of inluding data in the prompt to the LLM that was was not part of the language model training data. The overall vector store process looks like:\n",
    "\n",
    "![flow](https://python.langchain.com/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg)\n",
    "\n",
    "The components of the vector store RAG process:\n",
    "1. Document Loaders\n",
    "2. Document Transformers\n",
    "3. Text Embedding Models\n",
    "4. Vector Stores\n",
    "5. Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "These load documents from many different sources. The simpliest type of loader is `TextLoader`, which loads an entire file as a single document. The rest are common loaders you will commonly need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "  TextLoader, \n",
    "  DirectoryLoader, \n",
    "  UnstructuredHTMLLoader, \n",
    "  JSONLoader,\n",
    "  UnstructuredMarkdownLoader,\n",
    "  PyPDFLoader,\n",
    "  AsyncHtmlLoader,\n",
    "  WebBaseLoader\n",
    ")\n",
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextLoader(\"./data/data.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVLoader(\"./data/data.csv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirectoryLoader(\"./data/\", glob=\"*.txt\", loader_cls=TextLoader).load() # you can pick the loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnstructuredHTMLLoader(\"./data/data.html\").load() # will strip markup and load just the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONLoader(file_path=\"./data/data.json\", jq_schema=\".data[].name\").load() # returns text not json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnstructuredMarkdownLoader(\"./data/data.md\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPDFLoader(\"./data/data.pdf\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPDFLoader(\"./data/data_long.pdf\").load_and_split() # uses RecursiveCharacterTextSplitter to split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://shop.deere.com/us/product/Sherpa-Full-Zip-Jacket/p/SCUALC0593\"\n",
    "loader = AsyncHtmlLoader([url])\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(url) # a combination of a AsyncHtmlLoader and Html2TextLoader\n",
    "docs = loader.load() # there is also a loader.aload() for asyncronous loading\n",
    "print(docs[0].metadata)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Transformers\n",
    "\n",
    "After loading your documents, you will often want to transform them. The most common transformation is splitting the document into smaller chunks. Here are some common transformations:\n",
    "\n",
    "1. Text splitting\n",
    "2. Content Transformation\n",
    "3. Extract Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "from doctran import Doctran\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.document_transformers.openai_functions import create_metadata_tagger\n",
    "from langchain.schema import Document\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain_core.documents import BaseDocumentTransformer\n",
    "from langchain.document_transformers import (\n",
    "  Html2TextTransformer, \n",
    "  BeautifulSoupTransformer, \n",
    "  LongContextReorder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitting\n",
    "\n",
    "The most common text splitter is `RecursiveCharacterTextSplitter` which splits larger documents into smaller documents. There are a few things to keep mind when deciding how big each document (chunk) should be:\n",
    "\n",
    "1. You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
    "2. You want to have long enough documents that the context of each chunk is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=100, # maximium number of characters in a chunk (default: 4000)\n",
    "  chunk_overlap=20 # overlap between chunks to maintain context in each chunk (default: 200)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.split_documents(TextLoader(\"./data/sotu.txt\").load())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also be used directly on text\n",
    "RecursiveCharacterTextSplitter(chunk_size=3, chunk_overlap=1).split_text(\"this is some text 1 2 3 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple `CharacterTextSplitter` that doesn't use multiple separators to split the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CharacterTextSplitter(\n",
    "  separator=\"#ENTRY\",\n",
    "  chunk_size=10, # you will notice that the entries are still in their own document \n",
    "  chunk_overlap=5\n",
    ").split_documents(TextLoader(\"./data/data_split.txt\").load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://shop.deere.com/us/product/Sherpa-Full-Zip-Jacket/p/SCUALC0593\"\n",
    "loader = AsyncHtmlLoader(url)\n",
    "docs = loader.load()\n",
    "Html2TextTransformer().transform_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AsyncHtmlLoader(url)\n",
    "docs = loader.load()\n",
    "BeautifulSoupTransformer().transform_documents(\n",
    "  docs,\n",
    "  tags_to_extract=[\"main\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize\n",
    "\n",
    "There are a few ways to summarize multiple documents into one:\n",
    "\n",
    "1. `stuff`: All the documents will be put together and summarized by the LLM.\n",
    "2. `map_reduce`: Each document will be summarized by the LLM, and the then the LLM will summarize the summaries.\n",
    "3. `refine`: Will iterate over each document refining the summary until there are no more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sherpa = \"https://shop.deere.com/us/product/Sherpa-Full-Zip-Jacket/p/SCUALC0593\"\n",
    "puffer = \"https://shop.deere.com/us/product/Puffer-Jacket/p/SCUFLC0082\"\n",
    "loader = AsyncHtmlLoader([sherpa, puffer])\n",
    "docs = loader.load()\n",
    "transformed_docs = BeautifulSoupTransformer().transform_documents(\n",
    "  docs,\n",
    "  tags_to_extract=[\"main\"]\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need more control over the summary prompt, you can provide your own like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "Write a concise summary of the following, but DO NOT explicitly say it is a summary:\n",
    "\"{text}\"\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(summary_template)\n",
    "stuff_chain = load_summarize_chain(llm, \"stuff\", prompt=prompt)\n",
    "stuff_chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "Write a concise summary of the following, but DO NOT explicitly say it is a summary:\n",
    "\"{text}\"\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(summary_template)\n",
    "summary_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=summary_chain, document_variable_name=\"text\")\n",
    "stuff_chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a special chain that can summarize some arbitrary text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "analyze_chain = AnalyzeDocumentChain(combine_docs_chain=chain, text_splitter=text_splitter)\n",
    "analyze_chain.run(transformed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a library `doctran` that stands for Document Transformations that uses OpenAI to analyze documents and perform special tasks. One of those tasks is summarization. Currently, LangChain does not offer any wrapper for this doctran task. Here is how that is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2394"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(max_tokens=1000)\n",
    "output = llm(\"Provide a details about George Washinton's life.\")\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctran = Doctran()\n",
    "doc = doctran.parse(content=output)\n",
    "doc_summary = doc.summarize(token_limit=500).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = doc_summary.transformed_content\n",
    "len(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create our own custom document transformer that wraps `doctran`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeTransformer(BaseDocumentTransformer):\n",
    "  def transform_documents(\n",
    "    self, \n",
    "    documents: Sequence[Document], \n",
    "    token_limit: int = 200, \n",
    "    **kwargs: Any\n",
    "  ) -> Sequence[Document]:\n",
    "    doctran = Doctran()\n",
    "    transformed_docs = []\n",
    "    for doc in documents:\n",
    "      doc_summary = doctran.parse(content=doc.page_content).summarize(token_limit=token_limit).execute()\n",
    "      new_doc = Document(page_content=doc_summary.transformed_content)\n",
    "      transformed_docs.append(new_doc)\n",
    "    return transformed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummarizeTransformer().transform_documents([Document(page_content=output)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reordering (Lost-in-the-Middle)\n",
    "\n",
    "If there is too much context, some of it can get lost during inference. There are a couple ways of dealing with that. One, is to return less context. Two, sometimes just shuffling the context can also help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "  \"Call of the Wild (COTW) is a great, vast hunting game.\",\n",
    "  \"Way of the Hunter (WOTH) is a modern and good looking hunting game.\",\n",
    "  \"Metallica is a great heavy metal band.\",\n",
    "  \"VSCode is the best text editor for programming.\",\n",
    "  \"God of War is a great single player game.\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=x) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Call of the Wild (COTW) is a great, vast hunting game.'),\n",
       " Document(page_content='Way of the Hunter (WOTH) is a modern and good looking hunting game.'),\n",
       " Document(page_content='Metallica is a great heavy metal band.'),\n",
       " Document(page_content='VSCode is the best text editor for programming.'),\n",
       " Document(page_content='God of War is a great single player game.')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Call of the Wild (COTW) is a great, vast hunting game.'),\n",
       " Document(page_content='Metallica is a great heavy metal band.'),\n",
       " Document(page_content='God of War is a great single player game.'),\n",
       " Document(page_content='VSCode is the best text editor for programming.'),\n",
       " Document(page_content='Way of the Hunter (WOTH) is a modern and good looking hunting game.')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LongContextReorder().transform_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for relevant documents can be greatly improved with the proper metadata associated with the text itself. There are a couple ways that you can add metadata with LangChain.\n",
    "\n",
    "The first way is by using `OpenAI` functions, which tries to extract the metadata for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can define the schema with a dict or a pydantic model\n",
    "schema = {\n",
    "  \"properties\": {\n",
    "      \"category\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"the high-level category of the food being discussed like 'fruit' or 'vegetable' or snack\"\n",
    "      },\n",
    "      \"tone\": {\n",
    "        \"type\": \"string\",\n",
    "        \"enum\": [\"positive\", \"negative\"],\n",
    "        \"description\": \"the tone of the text like 'positive' or 'negative'\"\n",
    "      }\n",
    "  },\n",
    "  \"required\": [\"category\", \"tone\"],\n",
    "}\n",
    "llm = ChatOpenAI()\n",
    "document_transformer = create_metadata_tagger(llm=llm, metadata_schema=schema) # can provide prompt if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_documents = [\n",
    "  Document(page_content=\"The apples I ate were delicious.\"),\n",
    "  Document(page_content=\"The brocolli was bad.\"),\n",
    "]\n",
    "enhanced_documents = document_transformer.transform_documents(org_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The apples I ate were delicious.', metadata={'category': 'fruit', 'tone': 'positive'}),\n",
       " Document(page_content='The brocolli was bad.', metadata={'category': 'vegetable', 'tone': 'negative'})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectore Store of Embeddings\n",
    "\n",
    "It is very common for unstructured text to be stored as embeddings in a vector database, since it simplifies saving, searching, and retreiving text much easier. This vector store then becomes the primary data source for RAG. It is not the only source of data, but it is a common source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding Models\n",
    "\n",
    "These models can produce numerical embeddings of text. The purpose is so that retrieval can happen quickly with the much simplier vector space, which is a compressed representation of the text. There are a number of text embedding models that can be used. Here are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hug_model = HuggingFaceBgeEmbeddings(\n",
    "  model_name=\"BAAI/bge-small-en\",\n",
    "  model_kwargs={\"device\": \"cpu\"},\n",
    "  encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1536\n"
     ]
    }
   ],
   "source": [
    "embeddings = openai_model.embed_documents([\n",
    "  \"this is a test\",\n",
    "  \"another test\"\n",
    "])\n",
    "print(len(embeddings), len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = openai_model.embed_query(\"this is a test\") # single string\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = hug_model.embed_query(\"this is a test\")\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores\n",
    "\n",
    "There are a number of possible vector stores, but a few common local ones are `Chroma` and `FAISS`. A common online option is `Pinecone`. All of them implement the `VectorStore` interace. Let's walk through an example using `Chroma`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=\"Michael Jordan is the best basketball player of all time.\")]\n",
    "chroma = Chroma(embedding_function=openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_ids = chroma.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['816fd5d2-9abd-11ee-aa64-825fc74ed04f'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [None],\n",
       " 'documents': ['Michael Jordan is the best basketball player of all time.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma.get(record_ids) # has embedding but not returned in response by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Michael Jordan is the best basketball player of all time.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma.similarity_search(\"jordan\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Michael Jordan is the best basketball player of all time.'),\n",
       "  0.7929736264432757)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma.similarity_search_with_relevance_scores(\"jordan\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Michael Jordan is one of the best basketball player of all time.', metadata={'type': 'quote'})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximal marginal relevance (MMR) optimizes similarity and diversity\n",
    "chroma.max_marginal_relevance_search(\"jordan\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['816fd5d2-9abd-11ee-aa64-825fc74ed04f'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'type': 'quote'}],\n",
       " 'documents': ['Michael Jordan is one of the best basketball player of all time.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can update one or more documents\n",
    "# for some reason requires metadata in document\n",
    "chroma.update_document(\n",
    "  record_ids[0], \n",
    "  Document(page_content=\"Michael Jordan is one of the best basketball player of all time.\", metadata={\"type\": \"quote\"})\n",
    ")\n",
    "chroma.get(record_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is also possible to create the VectorStore client and add documents at the same time\n",
    "Chroma.from_texts([\"this is a test\"], openai_model) # from_documents also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreievers are able to query and retreive documents. Often a retriever uses a `VectorStore`, but not always. Moreover, retreivers are `Runnable`, which means they can be used as part of a `Chain`.\n",
    "\n",
    "Once we have the data loaded, transformed, and saved into a VectorStore, then we can retrieve relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, uuid, json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema import StrOutputParser, BaseOutputParser\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers.document_compressors import (\n",
    "  LLMChainExtractor, \n",
    "  LLMChainFilter, \n",
    "  EmbeddingsFilter,\n",
    "  CohereRerank\n",
    ")\n",
    "from langchain.retrievers import (\n",
    "  BM25Retriever, \n",
    "  EnsembleRetriever, \n",
    "  MultiQueryRetriever,\n",
    "  ContextualCompressionRetriever,\n",
    "  ParentDocumentRetriever,\n",
    "  MultiVectorRetriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "docs = TextLoader(\"./data/sotu.txt\").load()\n",
    "# transform\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "docs = splitter.split_documents(docs)\n",
    "# embed and save\n",
    "chroma = Chroma.from_documents(docs, openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve directly\n",
    "retriever = chroma.as_retriever(search_kwargs = {\"k\": 2}) # top 2 sources\n",
    "matching_docs = retriever.invoke(\"What did the president say about the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the relevant documents as part of a `chain` injecting the context into the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve as part of a chain\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}                                         \n",
    "\n",
    "Question: {question}                                          \n",
    "\"\"\")\n",
    "\n",
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "  return \"\\n\\n\".join([x.page_content for x in docs])\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "chain = (\n",
    "  {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "  | prompt | llm\n",
    ")\n",
    "chain.invoke(\"What did the president say about the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-query Retriever\n",
    "\n",
    "Instead of relying directly on the original query to retrieve the results, it can be beneficial to ask slightly different queries. The `MultiQueryRetriever` will provide that by doing multiple retrieval calls with subtle query changes and joining them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "chroma = Chroma(embedding_function=openai_model).as_retriever()\n",
    "retriever = MultiQueryRetriever.from_llm(chroma, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "retriever.get_relevant_documents(\"What did the president say about the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Query: \"What did the president say about the economy?\"\n",
    "\n",
    "Variations:\n",
    "1. Can you provide any information on the president's statements regarding the economy?\n",
    "2. What are the president's comments about the current state of the economy?\n",
    "3. I'm interested in knowing the president's views on the economy. Could you share any details?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Compression\n",
    "\n",
    "Sometimes the retrieved documents from a vector store may contain unneeded information that is unrelated to the query, or the document itself may just not be relevant to the query. Compression refers to the process of only keeping the relevant part of the document and relevent documents. This is often needed since LLMs will have input size limits.\n",
    "\n",
    "First, let's see just the common compressor `LLMChainExtractor`, which extract important parts of the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\n",
    "  \"We’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees.\",\n",
    "   \"Let’s pass the Paycheck Fairness Act and paid leave.\"\n",
    "]\n",
    "docs = [Document(page_content=x) for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compressed_docs = compressor.compress_documents(docs, \"fairness act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly different version of the compressor is the `LLMChainFilter`, which will remove the documents that are not important to the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainFilter.from_llm(llm)\n",
    "compressed_docs = compressor.compress_documents(docs, \"paid leave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far we have seen compressors that rely upon the LLM to perform the work. There is also the `EmbeddingsFilter` which will use the embedding model to compare the documents and remove those that fall under a threshold. This approach is much faster than using the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressor = EmbeddingsFilter(embeddings=openai_model, similarity_threshold=0.80)\n",
    "compressed_docs = compressor.compress_documents(docs, \"fairness act\")\n",
    "len(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods can produce good results, but the best way to compress is to rerank the documents using a dedicated reranking model, like the one provided by `Cohere` or an open source model. The reason why this is better is because it is faster than using a LLM and it maintains full semantic context unlike embeddings which lose fidelity during the embedding process. Here is how to do it with Cohere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = CohereRerank()\n",
    "rerank.compress_documents(docs, \"fairness act\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine the compressor with a retriever to see how it works. This is done using the `ContextualCompressionRetriever`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = Chroma(embedding_function=openai_model).as_retriever()\n",
    "retriever = ContextualCompressionRetriever(base_compressor=rerank, base_retriever=chroma)\n",
    "retriever.get_relevant_documents(\"What did the president say about the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever\n",
    "\n",
    "From a high-level this retriever simply uses multiple retrievers, combines their results, reranks them using Reciprocal Rank Fusion and returns a single list of documents. Practically, this approach is normally used with diverse types of retrievers. For example, one can use a sparse database like OpenSearch or any BM25 algorithm and a dense database like the vector stores we have been using. This type of strategy is called a \"hybrid search\". We will use BM25 and Chroma as our retrievers in this example.\n",
    "\n",
    "> Note: The Reciprocal Rank Fusion (RRF) algorithm is a simple but effective way to combine lists from multiple sources, where each source has its own way of scoring. In other words, RRF creates a new scoring strategy by combining disparate scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "  \"I like apples\",\n",
    "  \"I like oranges\",\n",
    "  \"Apples and oranges are fruits\"\n",
    "]\n",
    "bm25 = BM25Retriever.from_texts(docs)\n",
    "bm25.k = 2\n",
    "chroma = Chroma.from_texts(docs, openai_model).as_retriever(search_kwargs = {\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EnsembleRetriever(\n",
    "  retrievers=[bm25, chroma],\n",
    "  weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples'),\n",
       " Document(page_content='Apples and oranges are fruits')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"apples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent Document Retriever\n",
    "\n",
    "When storing documents into a vector store, you will usually have multiple smaller documents (chunks). This makes search faster and more effective; however, during retrieval, you may want to have the original document the chunk originated from. That is what the `ParentDocumentRetriever` is used for. Keep in mind, if you intend on retrieving the original documents, then you need to save them using this retriever also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a single document, two documents get created due to the splitting\n",
    "splitter = CharacterTextSplitter(\n",
    "  separator=\"#ENTRY\",\n",
    "  chunk_size=10,\n",
    "  chunk_overlap=5\n",
    ")\n",
    "docs = TextLoader(\"./data/data_split.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content) # original document size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove collection if exists already\n",
    "Chroma(collection_name=\"full_documents\").delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = Chroma(collection_name=\"full_documents\", embedding_function=openai_model)\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "  vectorstore=chroma, \n",
    "  docstore=store, \n",
    "  child_splitter=splitter,\n",
    "  search_kwargs={\"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever splits and stores with document metadata\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87e834e7-be05-4ca1-a684-2ba26189c4d8']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(store.yield_keys()) # only one source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'doc_id': '87e834e7-be05-4ca1-a684-2ba26189c4d8',\n",
       " 'source': './data/data_split.txt'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = chroma.similarity_search(\"entry\", k=2)\n",
    "print(len(docs)) # result is two documents\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 144)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns one document event though there are two documents in vector store\n",
    "found_docs = retriever.get_relevant_documents(\"entry\")\n",
    "(len(found_docs), len(found_docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Vector Retriever\n",
    "\n",
    "We actually just saw a multi-vector retriever: `ParentDocumentRetriever`. In fact, that class implements the `MultiVectorRetriever` class. We'll use this base class to satisfy a couple use-caes that benefit from multiple vectors.\n",
    "\n",
    "#### Summary\n",
    "The first use-case is the desire to search the summary of a document, but still return the original document when retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=200,\n",
    "  chunk_overlap=10\n",
    ")\n",
    "docs = splitter.split_documents(TextLoader(\"./data/sotu.txt\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "chain = (\n",
    "  {\"doc\": lambda x: x.page_content}\n",
    "  | summary_prompt\n",
    "  | ChatOpenAI(max_retries=0)\n",
    "  | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Org: 200 Summary: 121\n"
     ]
    }
   ],
   "source": [
    "print(\"Org:\", len(docs[0].page_content), \"Summary:\", len(summaries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chroma(collection_name=\"summaries\").delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = Chroma(collection_name=\"summaries\", embedding_function=openai_model)\n",
    "store = InMemoryStore() # storage for parent documents\n",
    "id_key = \"doc_id\"\n",
    "retriever = MultiVectorRetriever(\n",
    "  vectorstore=chroma, \n",
    "  docstore=store, \n",
    "  id_key=id_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give each summary document a unique document ID\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "summary_docs = [\n",
    "  Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "  for i, s in enumerate(summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['fdc7e216-9b92-11ee-9647-825fc74ed051'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'doc_id': 'c6de8e6b-6323-4ba6-a7ec-a217e5cc9bb9'}],\n",
       " 'documents': ['On February 4, 2020, President Donald J. Trump delivered a State of the Union address before a joint session of Congress.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_ids = retriever.vectorstore.add_documents(summary_docs)\n",
    "chroma.get(vector_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will save the second vector to storage, which relates to the embedded vector metadata\n",
    "doc_ids_to_doc = list(zip(doc_ids, docs))\n",
    "retriever.docstore.mset(doc_ids_to_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'doc_id': '39f921df-7703-4d09-872a-7b17d2632758'}, 937)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the summarized version of the document\n",
    "result = chroma.similarity_search(\"economic\")[0]\n",
    "summary_doc_id = result.metadata[\"doc_id\"]\n",
    "(result.metadata, len(result.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('39f921df-7703-4d09-872a-7b17d2632758', {'source': './data/sotu.txt'}, 199)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the orignal document before summarizing\n",
    "result = retriever.get_relevant_documents(\"economic\")[0]\n",
    "(summary_doc_id, result.metadata, len(result.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothetical Queries\n",
    "\n",
    "An LLM chat conversation is usually a series of questions from the human and answers from the LLM. Therefore, it can be beneficial to understand what types of questions the document in a RAG setup can answer. In other words, during RAG we want to search for relevant documents based off of the potential questions the document can answer. That is what this example use-case of `MultiVectorRetriever` will demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonOutputParser(BaseOutputParser):\n",
    "  def parse(self, text: str) -> list:\n",
    "    try:\n",
    "      return json.loads(text)\n",
    "    except:\n",
    "      return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"\n",
    "Respond in JSON format. The response should only be the array and should not be an object. \n",
    "\n",
    "Here is an exmaple response:\n",
    "[\"Question 1?\", \"Question 2?\", \"Question 3?\"]\n",
    "\n",
    "Generate a JSON list of exactly 3 hypothetical questions that the below document could be used to answer:\n",
    "{doc}\n",
    "\"\"\"\n",
    "questions_prompt = ChatPromptTemplate.from_template(question_template)\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | questions_prompt\n",
    "    | ChatOpenAI(max_retries=0, model=\"gpt-3.5-turbo\")\n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What was the date of Donald J. Trump's State of the Union address in 2020?\",\n",
       " \"What is the title of Donald J. Trump's speech before a Joint Session of Congress in 2020?\",\n",
       " 'How did Donald J. Trump open his speech before a Joint Session of Congress in 2020?']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 279, 837)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(docs), len(questions), 3 * len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chroma(collection_name=\"questions\").delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = Chroma(collection_name=\"questions\", embedding_function=openai_model)\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "retriever = MultiVectorRetriever(\n",
    "  vectorstore=chroma, \n",
    "  docstore=store, \n",
    "  id_key=id_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "# every question becomes it's own document associated with the original document\n",
    "question_docs = []\n",
    "for i, question_list in enumerate(questions):\n",
    "  question_docs.extend([\n",
    "    Document(page_content=q, metadata={id_key: doc_ids[i]})\n",
    "    for q in question_list\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(question_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f46ebe1a-0af1-463a-90d3-84d3df38572e',\n",
       " 41,\n",
       " 'What is the current state of the economy?')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the question that is actually being matched\n",
    "result = chroma.similarity_search(\"economic\")[0]\n",
    "summary_doc_id = result.metadata[\"doc_id\"]\n",
    "(result.metadata[\"doc_id\"], len(result.page_content), result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f46ebe1a-0af1-463a-90d3-84d3df38572e', {'source': './data/sotu.txt'}, 189)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the original document the question was generated from\n",
    "result = retriever.get_relevant_documents(\"economic\")[0]\n",
    "(summary_doc_id, result.metadata, len(result.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Querying Vector Stores\n",
    "\n",
    "There is another powerful feature of many vector stores like `Chroma` that we haven't used: metadata. This metadata can be helpful to filter the vector store to improve search results. You could filter on metadata yourself, but this makes the process easier. The `SelfQueryRetriever` class requires the package `lark` to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some test documents about movie reviews\n",
    "docs = [\n",
    "  Document(\n",
    "    page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "    metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "  ),\n",
    "  Document(\n",
    "    page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "    metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "  ),\n",
    "  Document(\n",
    "    page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "    metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "  ),\n",
    "  Document(\n",
    "    page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "    metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "  ),\n",
    "  Document(\n",
    "    page_content=\"Toys come alive and have a blast doing so\",\n",
    "    metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "  ),\n",
    "  Document(\n",
    "    page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "    metadata={\n",
    "      \"year\": 1979,\n",
    "      \"director\": \"Andrei Tarkovsky\",\n",
    "      \"genre\": \"thriller\",\n",
    "      \"rating\": 9.9,\n",
    "    },\n",
    "  ),\n",
    "]\n",
    "chroma = Chroma.from_documents(docs, openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions of the metadata fields\n",
    "metadata_field_info = [\n",
    "  AttributeInfo(\n",
    "    name=\"genre\",\n",
    "    description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "    type=\"string\"\n",
    "  ),\n",
    "  AttributeInfo(\n",
    "    name=\"year\",\n",
    "    description=\"The year the movie was released\",\n",
    "    type=\"integer\"\n",
    "  ),\n",
    "  AttributeInfo(\n",
    "    name=\"director\",\n",
    "    description=\"The name of the movie director\",\n",
    "    type=\"string\"\n",
    "  ),\n",
    "  AttributeInfo(\n",
    "    name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "  )\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "  llm,\n",
    "  chroma,\n",
    "  document_content_description,\n",
    "  metadata_field_info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'director': 'Andrei Tarkovsky',\n",
       "  'genre': 'thriller',\n",
       "  'rating': 9.9,\n",
       "  'year': 1979},\n",
       " {'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# querying only by filter\n",
    "results = retriever.invoke(\"I want to watch a movie rated higher than 8.5\")\n",
    "[r.metadata for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'director': 'Greta Gerwig', 'rating': 8.3, 'year': 2019}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# querying by document content and filter\n",
    "results = retriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n",
    "[r.metadata for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Research Retriever\n",
    "\n",
    "This is a cool, but a bit restrictive retriever that will take a question, ask the LLM to generate 3 questions optimized for search, search Google, store the results in a vector store, search the vector store for the original question, and include the relevant documents (RAG) along with the original question to the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires the `google-api-python-client` and `html2text` package since it uses Google Search directly.\n",
    "\n",
    "> Note: having the source returned does not appear to be working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.utilities.google_search import GoogleSearchAPIWrapper\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources import stuff_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenAIEmbeddings()\n",
    "Chroma(collection_name=\"web_research\").delete_collection()\n",
    "chroma = Chroma(embedding_function=openai_model, collection_name=\"web_research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "  vectorstore=chroma,\n",
    "  llm=llm,\n",
    "  search=search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how retriever fetches documents from the web\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "web_docs = web_research_retriever.get_relevant_documents(user_input)\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# Agent System Overview#\\n\\nIn a LLM-powered autonomous agent system, LLM functions as the agent's brain,\\ncomplemented by several key components:\\n\\n  * **Planning**\\n    * Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\n    * Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n  * **Memory**\\n    * Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\n    * Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n  * **Tool use**\\n    * The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\n\\n# Component One: Planning#\\n\\nA complicated task usually involves many steps. An agent needs to know what\\nthey are and plan ahead.\\n\\n## Task Decomposition#\", metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content='The design of generative agents combines LLM with memory, planning and\\nreflection mechanisms to enable agents to behave conditioned on past\\nexperience, as well as to interact with other agents.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content='Boiko et al. (2023) also looked into LLM-empowered agents for scientific\\ndiscovery, to handle autonomous design, planning, and performance of complex\\nscientific experiments. This agent can use tools to browse the Internet, read\\ndocumentation, execute code, call robotics experimentation APIs and leverage\\nother LLMs.\\n\\nFor example, when requested to `\"develop a novel anticancer drug\"`, the model\\ncame up with the following reasoning steps:\\n\\n  1. inquired about current trends in anticancer drug discovery;\\n  2. selected a target;\\n  3. requested a scaffold targeting these compounds;\\n  4. Once the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons.\\nThey developed a test set containing a list of known chemical weapon agents\\nand asked the agent to synthesize them. 4 out of 11 requests (36%) were\\naccepted to obtain a synthesis solution and the agent attempted to consult\\ndocumentation to execute the procedure. 7 out of 11 were rejected and among\\nthese 7 rejected cases, 5 happened after a Web search while 2 were rejected\\nbased on prompt only.\\n\\n## Generative Agents Simulation#\\n\\n**Generative Agents** (Park, et al. 2023) is super fun experiment where 25\\nvirtual characters, each controlled by a LLM-powered agent, are living and\\ninteracting in a sandbox environment, inspired by The Sims. Generative agents\\ncreate believable simulacra of human behavior for interactive applications.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content=\"Lil'Log\\n\\n  * Posts\\n  * Archive\\n  * Search\\n  * Tags\\n  * FAQ\\n  * emojisearch.app\\n\\n#  LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng\\n\\nTable of Contents\\n\\n  * Agent System Overview\\n  * Component One: Planning\\n    * Task Decomposition\\n    * Self-Reflection\\n  * Component Two: Memory\\n    * Types of Memory\\n    * Maximum Inner Product Search (MIPS)\\n  * Component Three: Tool Use\\n  * Case Studies\\n    * Scientific Discovery Agent\\n    * Generative Agents Simulation\\n    * Proof-of-Concept Examples\\n  * Challenges\\n  * Citation\\n  * References\\n\\nBuilding agents with LLM (large language model) as its core controller is a\\ncool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer\\nand BabyAGI, serve as inspiring examples. The potentiality of LLM extends\\nbeyond generating well-written copies, stories, essays and programs; it can be\\nframed as a powerful general problem solver.\\n\\n# Agent System Overview#\\n\\nIn a LLM-powered autonomous agent system, LLM functions as the agent's brain,\\ncomplemented by several key components:\", metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['373fb67c-9c97-11ee-87ab-825fc74ed051'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:',\n",
       "   'language': 'en',\n",
       "   'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'title': \"LLM Powered Autonomous Agents | Lil'Log\"}],\n",
       " 'documents': [\"Lil'Log\\n\\n  * Posts\\n  * Archive\\n  * Search\\n  * Tags\\n  * FAQ\\n  * emojisearch.app\\n\\n#  LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng\\n\\nTable of Contents\\n\\n  * Agent System Overview\\n  * Component One: Planning\\n    * Task Decomposition\\n    * Self-Reflection\\n  * Component Two: Memory\\n    * Types of Memory\\n    * Maximum Inner Product Search (MIPS)\\n  * Component Three: Tool Use\\n  * Case Studies\\n    * Scientific Discovery Agent\\n    * Generative Agents Simulation\\n    * Proof-of-Concept Examples\\n  * Challenges\\n  * Citation\\n  * References\\n\\nBuilding agents with LLM (large language model) as its core controller is a\\ncool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer\\nand BabyAGI, serve as inspiring examples. The potentiality of LLM extends\\nbeyond generating well-written copies, stories, essays and programs; it can be\\nframed as a powerful general problem solver.\\n\\n# Agent System Overview#\\n\\nIn a LLM-powered autonomous agent system, LLM functions as the agent's brain,\\ncomplemented by several key components:\"],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how retriever stores documents in the vector store\n",
    "chroma.get(chroma.get()[\"ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "  llm=llm,\n",
    "  retriever=web_research_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"question\": user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How do LLM Powered Autonomous Agents work?',\n",
       " 'answer': \"LLM-powered autonomous agents work by using LLM as the agent's brain, complemented by several key components such as planning, memory, and tool use. The planning component involves task decomposition and self-reflection. The memory component includes short-term memory and long-term memory. The tool use component allows the agent to call external APIs for additional information. LLM-powered agents have been used in various applications, including scientific discovery and generative agents simulation. They have the potential to be powerful general problem solvers. \\n\",\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging what is happening inside of `RetrievalQAWithSourcesChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuft_chain = StuffDocumentsChain(\n",
    "  llm_chain=LLMChain(llm=llm, prompt=stuff_prompt.PROMPT),\n",
    "  document_variable_name=\"summaries\",\n",
    "  document_prompt=stuff_prompt.EXAMPLE_PROMPT \n",
    ")\n",
    "combined_docs = stuft_chain.run(input_documents=web_docs, question=user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"LLM-powered autonomous agents work by using LLM as the agent's brain, complemented by several key components such as planning, memory, and tool use. The planning component involves task decomposition and self-reflection. The memory component includes short-term memory and long-term memory. The tool use component allows the agent to call external APIs for additional information. LLM-powered agents have been used in various applications, including scientific discovery and generative agents simulation. They have the potential to be powerful general problem solvers. \\nSOURCES: \\n- https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
       " \"LLM-powered autonomous agents work by using LLM as the agent's brain, complemented by several key components such as planning, memory, and tool use. The planning component involves task decomposition and self-reflection. The memory component includes short-term memory and long-term memory. The tool use component allows the agent to call external APIs for additional information. LLM-powered agents have been used in various applications, including scientific discovery and generative agents simulation. They have the potential to be powerful general problem solvers. \\n\",\n",
       " ' \\n- https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " '')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "answer, sources = re.split(r\"SOURCES?:|QUESTION:\\s\", combined_docs, flags=re.IGNORECASE)[:2]\n",
    "source = re.split(r\"\\n\", sources)[0].strip()\n",
    "(combined_docs, answer, sources, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever Tool within Agent\n",
    "\n",
    "What if we want an agent to be able to also use RAG? Yes, we can create a tool that uses a retriever `chain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool, create_conversational_retrieval_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = Chroma(embedding_function=openai_model, collection_name=\"web_research\").as_retriever()\n",
    "# a helper function to create a retriever Tool\n",
    "tool = create_retriever_tool(chroma, \"search_internet\", \"Will search the internet for relevant documents.\")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_conversational_retrieval_agent(llm, tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent({\"input\": \"What is an LLM agent?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
