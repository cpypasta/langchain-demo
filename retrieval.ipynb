{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is the process of inluding data in the prompt to the LLM that was was not part of the language model training data. The overall vector store process looks like:\n",
    "\n",
    "![flow](https://python.langchain.com/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg)\n",
    "\n",
    "The components of the RAG process:\n",
    "1. Document Loaders\n",
    "2. Document Transformers\n",
    "3. Text Embedding Models\n",
    "4. Vector Stores\n",
    "5. Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "These load documents from many different sources. The simpliest type of loader is `TextLoader`, which loads an entire file as a single document. The rest are common loaders you will commonly need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "  TextLoader, \n",
    "  DirectoryLoader, \n",
    "  UnstructuredHTMLLoader, \n",
    "  JSONLoader,\n",
    "  UnstructuredMarkdownLoader,\n",
    "  PyPDFLoader,\n",
    "  AsyncHtmlLoader,\n",
    "  WebBaseLoader\n",
    ")\n",
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextLoader(\"./data/data.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVLoader(\"./data/data.csv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirectoryLoader(\"./data/\", glob=\"*.txt\", loader_cls=TextLoader).load() # you can pick the loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnstructuredHTMLLoader(\"./data/data.html\").load() # will strip markup and load just the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONLoader(file_path=\"./data/data.json\", jq_schema=\".data[].name\").load() # returns text not json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnstructuredMarkdownLoader(\"./data/data.md\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPDFLoader(\"./data/data.pdf\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyPDFLoader(\"./data/data_long.pdf\").load_and_split() # uses RecursiveCharacterTextSplitter to split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://shop.deere.com/us/product/Sherpa-Full-Zip-Jacket/p/SCUALC0593\"\n",
    "loader = AsyncHtmlLoader([url])\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(url) # a combination of a AsyncHtmlLoader and Html2TextLoader\n",
    "docs = loader.load() # there is also a loader.aload() for asyncronous loading\n",
    "print(docs[0].metadata)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Transformers\n",
    "\n",
    "After loading your documents, you will often want to transform them. The most common transformation is splitting the document into smaller chunks. Here are some common transformations:\n",
    "\n",
    "1. Text splitting\n",
    "2. Content Transformation\n",
    "3. Extract Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "from doctran import Doctran\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.document_transformers.openai_functions import create_metadata_tagger\n",
    "from langchain.schema import Document\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain_core.documents import BaseDocumentTransformer\n",
    "from langchain.document_transformers import (\n",
    "  Html2TextTransformer, \n",
    "  BeautifulSoupTransformer, \n",
    "  LongContextReorder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitting\n",
    "\n",
    "The most common text splitter is `RecursiveCharacterTextSplitter` which splits larger documents into smaller documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=100, # maximium number of characters in a chunk (default: 4000)\n",
    "  chunk_overlap=20 # overlap between chunks to maintain context in each chunk (default: 200)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.split_documents(TextLoader(\"./data/sotu.txt\").load())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also be used directly on text\n",
    "RecursiveCharacterTextSplitter(chunk_size=3, chunk_overlap=1).split_text(\"this is some text 1 2 3 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple `CharacterTextSplitter` that doesn't use multiple separators to split the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CharacterTextSplitter(\n",
    "  separator=\"#ENTRY\",\n",
    "  chunk_size=10, # you will notice that the entries are still in their own document \n",
    "  chunk_overlap=5\n",
    ").split_documents(TextLoader(\"./data/data_split.txt\").load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://shop.deere.com/us/product/Sherpa-Full-Zip-Jacket/p/SCUALC0593\"\n",
    "loader = AsyncHtmlLoader(url)\n",
    "docs = loader.load()\n",
    "Html2TextTransformer().transform_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AsyncHtmlLoader(url)\n",
    "docs = loader.load()\n",
    "BeautifulSoupTransformer().transform_documents(\n",
    "  docs,\n",
    "  tags_to_extract=[\"main\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize\n",
    "\n",
    "There are a few ways to summarize documents:\n",
    "\n",
    "1. `stuff`: All the documents will be put together and summarized by the LLM.\n",
    "2. `map_reduce`: Each document will be summarized by the LLM, and the then the LLM will summarize the summaries.\n",
    "3. `refine`: Will iterate over each document refining the summary until there are no more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sherpa = \"https://shop.deere.com/us/product/Sherpa-Full-Zip-Jacket/p/SCUALC0593\"\n",
    "puffer = \"https://shop.deere.com/us/product/Puffer-Jacket/p/SCUFLC0082\"\n",
    "loader = AsyncHtmlLoader([sherpa, puffer])\n",
    "docs = loader.load()\n",
    "transformed_docs = BeautifulSoupTransformer().transform_documents(\n",
    "  docs,\n",
    "  tags_to_extract=[\"main\"]\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need more control over the summary prompt, you can provide your own like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "Write a concise summary of the following, but DO NOT explicitly say it is a summary:\n",
    "\"{text}\"\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(summary_template)\n",
    "stuff_chain = load_summarize_chain(llm, \"stuff\", prompt=prompt)\n",
    "stuff_chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\n",
    "Write a concise summary of the following, but DO NOT explicitly say it is a summary:\n",
    "\"{text}\"\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(summary_template)\n",
    "summary_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=summary_chain, document_variable_name=\"text\")\n",
    "stuff_chain.run(transformed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a special chain that can summarize some arbitrary text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "analyze_chain = AnalyzeDocumentChain(combine_docs_chain=chain, text_splitter=text_splitter)\n",
    "analyze_chain.run(transformed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a library `doctran` that stands for Document Transformations that uses OpenAI to analyze documents and perform special tasks. One of those tasks is summarization. Currently, LangChain does not offer any wrapper for this doctran task. Here is how that is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2394"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(max_tokens=1000)\n",
    "output = llm(\"Provide a details about George Washinton's life.\")\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctran = Doctran()\n",
    "doc = doctran.parse(content=output)\n",
    "doc_summary = doc.summarize(token_limit=500).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = doc_summary.transformed_content\n",
    "len(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create our own custom document transformer that wraps `doctran`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeTransformer(BaseDocumentTransformer):\n",
    "  def transform_documents(\n",
    "    self, \n",
    "    documents: Sequence[Document], \n",
    "    token_limit: int = 200, \n",
    "    **kwargs: Any\n",
    "  ) -> Sequence[Document]:\n",
    "    doctran = Doctran()\n",
    "    transformed_docs = []\n",
    "    for doc in documents:\n",
    "      doc_summary = doctran.parse(content=doc.page_content).summarize(token_limit=token_limit).execute()\n",
    "      new_doc = Document(page_content=doc_summary.transformed_content)\n",
    "      transformed_docs.append(new_doc)\n",
    "    return transformed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummarizeTransformer().transform_documents([Document(page_content=output)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reordering (Lost-in-the-Middle)\n",
    "\n",
    "If there is too much context, some of it can get lost during inference. There are a couple ways of dealing with that. One, is to return less context. Two, sometimes just shuffling the context can also help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "  \"Call of the Wild (COTW) is a great, vast hunting game.\",\n",
    "  \"Way of the Hunter (WOTH) is a modern and good looking hunting game.\",\n",
    "  \"Metallica is a great heavy metal band.\",\n",
    "  \"VSCode is the best text editor for programming.\",\n",
    "  \"God of War is a great single player game.\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=x) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Call of the Wild (COTW) is a great, vast hunting game.'),\n",
       " Document(page_content='Way of the Hunter (WOTH) is a modern and good looking hunting game.'),\n",
       " Document(page_content='Metallica is a great heavy metal band.'),\n",
       " Document(page_content='VSCode is the best text editor for programming.'),\n",
       " Document(page_content='God of War is a great single player game.')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Call of the Wild (COTW) is a great, vast hunting game.'),\n",
       " Document(page_content='Metallica is a great heavy metal band.'),\n",
       " Document(page_content='God of War is a great single player game.'),\n",
       " Document(page_content='VSCode is the best text editor for programming.'),\n",
       " Document(page_content='Way of the Hunter (WOTH) is a modern and good looking hunting game.')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LongContextReorder().transform_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for relevant documents can be greatly improved with the proper metadata associated with the text itself. There are a couple ways that you can add metadata with LangChain.\n",
    "\n",
    "The first way is by using `OpenAI` functions, which tries to extract the metadata for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can define the schema with a dict or a pydantic model\n",
    "schema = {\n",
    "  \"properties\": {\n",
    "      \"category\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"the high-level category of the food being discussed like 'fruit' or 'vegetable' or snack\"\n",
    "      },\n",
    "      \"tone\": {\n",
    "        \"type\": \"string\",\n",
    "        \"enum\": [\"positive\", \"negative\"],\n",
    "        \"description\": \"the tone of the text like 'positive' or 'negative'\"\n",
    "      }\n",
    "  },\n",
    "  \"required\": [\"category\", \"tone\"],\n",
    "}\n",
    "llm = ChatOpenAI()\n",
    "document_transformer = create_metadata_tagger(llm=llm, metadata_schema=schema) # can provide prompt if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_documents = [\n",
    "  Document(page_content=\"The apples I ate were delicious.\"),\n",
    "  Document(page_content=\"The brocolli was bad.\"),\n",
    "]\n",
    "enhanced_documents = document_transformer.transform_documents(org_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The apples I ate were delicious.', metadata={'category': 'fruit', 'tone': 'positive'}),\n",
       " Document(page_content='The brocolli was bad.', metadata={'category': 'vegetable', 'tone': 'negative'})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectore Store of Embeddings\n",
    "\n",
    "It is very common for unstructured text to be stored as embeddings in a vector database, since it simplifies saving, searching, and retreiving text much easier. This vector store then becomes the primary data source for RAG. It is not the only source of data, but it is a common source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding Models\n",
    "\n",
    "These models can produce numerical embeddings of text. The purpose is so that retrieval can happen quickly with the much simplier vector space, which is a compressed representation of the text. There are a number of text embedding models that can be used. Here are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenAIEmbeddings()\n",
    "hug_model = HuggingFaceBgeEmbeddings(\n",
    "  model_name=\"BAAI/bge-small-en\",\n",
    "  model_kwargs={\"device\": \"cpu\"},\n",
    "  encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1536\n"
     ]
    }
   ],
   "source": [
    "embeddings = openai_model.embed_documents([\n",
    "  \"this is a test\",\n",
    "  \"another test\"\n",
    "])\n",
    "print(len(embeddings), len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = openai_model.embed_query(\"this is a test\") # single string\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = hug_model.embed_query(\"this is a test\")\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores\n",
    "\n",
    "There are a number of possible vector stores, but a few common local ones are `Chroma` and `FAISS`. A common online option is `Pinecone`. All of them implement the `VectorStore` interace. Let's walk through an example using `Chroma`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=\"Michael Jordan is the best basketball player of all time.\")]\n",
    "chroma = Chroma(embedding_function=openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_ids = chroma.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['816fd5d2-9abd-11ee-aa64-825fc74ed04f'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [None],\n",
       " 'documents': ['Michael Jordan is the best basketball player of all time.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma.get(record_ids) # has embedding but not returned by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Michael Jordan is the best basketball player of all time.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma.similarity_search(\"jordan\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Michael Jordan is the best basketball player of all time.'),\n",
       "  0.7929736264432757)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma.similarity_search_with_relevance_scores(\"jordan\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Michael Jordan is one of the best basketball player of all time.', metadata={'type': 'quote'})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximal marginal relevance (MMR) optimizes similarity and diversity\n",
    "chroma.max_marginal_relevance_search(\"jordan\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['816fd5d2-9abd-11ee-aa64-825fc74ed04f'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'type': 'quote'}],\n",
       " 'documents': ['Michael Jordan is one of the best basketball player of all time.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can update one or more documents\n",
    "# for some reason requires metadata in document\n",
    "chroma.update_document(\n",
    "  record_ids[0], \n",
    "  Document(page_content=\"Michael Jordan is one of the best basketball player of all time.\", metadata={\"type\": \"quote\"})\n",
    ")\n",
    "chroma.get(record_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is also possible to create the VectorStore client and add documents at the same time\n",
    "Chroma.from_texts([\"this is a test\"], openai_model) # from_documents also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreievers are able to query and retreive documents. Often a retriever uses a VectorStore, but not always. Moreover, retreivers are `Runnable`, which means they can be used as part of a `Chain`.\n",
    "\n",
    "Once we have the data loaded, transformed, and saved into a VectorStore, then retrieve relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers.document_compressors import (\n",
    "  LLMChainExtractor, \n",
    "  LLMChainFilter, \n",
    "  EmbeddingsFilter,\n",
    "  CohereRerank\n",
    ")\n",
    "from langchain.retrievers import (\n",
    "  BM25Retriever, \n",
    "  EnsembleRetriever, \n",
    "  MultiQueryRetriever,\n",
    "  ContextualCompressionRetriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "docs = TextLoader(\"./data/sotu.txt\").load()\n",
    "# transform\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "docs = splitter.split_documents(docs)\n",
    "# embed and save\n",
    "chroma = Chroma.from_documents(docs, openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve directly\n",
    "retriever = chroma.as_retriever(search_kwargs = {\"k\": 2}) # top 2 sources\n",
    "matching_docs = retriever.invoke(\"What did the president say about the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the relevant documents as part of a `chain` injecting the context into the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve as part of a chain\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}                                         \n",
    "\n",
    "Question: {question}                                          \n",
    "\"\"\")\n",
    "\n",
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "  return \"\\n\\n\".join([x.page_content for x in docs])\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "chain = (\n",
    "  {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "  | prompt | llm\n",
    ")\n",
    "chain.invoke(\"What did the president say about the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-query Retriever\n",
    "\n",
    "Instead of relying directly on the original query to retrieve the results, it can be beneficial to ask slightly different queries. The `MultiQueryRetriever` will provide that by doing multiple retrieval calls with subtle changes and joining them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "chroma = Chroma(embedding_function=openai_model).as_retriever()\n",
    "retriever = MultiQueryRetriever.from_llm(chroma, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "retriever.get_relevant_documents(\"What did the president say about the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Query: \"What did the president say about the economy?\"\n",
    "\n",
    "Variations:\n",
    "1. Can you provide any information on the president's statements regarding the economy?\n",
    "2. What are the president's comments about the current state of the economy?\n",
    "3. I'm interested in knowing the president's views on the economy. Could you share any details?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Compression\n",
    "\n",
    "Sometimes the retrieved documents from a vector store may contain unneeded information that is unrelated to the query, or the document itself may just not be relevant to the query. Compression refers to the process of only keeping the relevant part of the document and relevent documents.\n",
    "\n",
    "First, let's see just the common compressor `LLMChainExtractor`, which extract important parts of the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\n",
    "  \"We’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees.\",\n",
    "   \"Let’s pass the Paycheck Fairness Act and paid leave.\"\n",
    "]\n",
    "docs = [Document(page_content=x) for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compressed_docs = compressor.compress_documents(docs, \"fairness act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='giving workers a fair shot'),\n",
       " Document(page_content='Let’s pass the Paycheck Fairness Act')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly different version of the compressor is the `LLMChainFilter`, which will remove the documents that are not important to the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainFilter.from_llm(llm)\n",
    "compressed_docs = compressor.compress_documents(docs, \"fairness act\")\n",
    "len(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far we have seen compressors that rely upon the LLM to perform the work. There is also the `EmbeddingsFilter` which will use the embeddings to compare the documents and remove those that fall under a threshold. This approach is much faster than using the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressor = EmbeddingsFilter(embeddings=openai_model, similarity_threshold=0.80)\n",
    "compressed_docs = compressor.compress_documents(docs, \"fairness act\")\n",
    "len(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods have been good, but the best way to compress is to rerank the documents using a dedicated reranking model, like the one provided by `Cohere` or an open source model. The reason why this is better is because it is faster than using a LLM and it maintains full semantic context unlike embeddings which lose fidelity during the embedding process. Here is how to do it with Cohere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = CohereRerank()\n",
    "rerank.compress_documents(docs, \"fairness act\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine the compressor with a retriever to see how it works. This is done using the `ContextualCompressionRetriever`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = Chroma(embedding_function=openai_model).as_retriever()\n",
    "retriever = ContextualCompressionRetriever(base_compressor=rerank, base_retriever=chroma)\n",
    "retriever.get_relevant_documents(\"What did the president say about the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever\n",
    "\n",
    "From a high-level this retriever simply uses multiple retrievers, combines their results, reranks them using Reciprocal Rank Fusion and returns a single list of documents. Practically, this approach is normally used with diverse types of retrievers. For example, one can use a sparse database like OpenSearch or any BM25 algorithm and a dense database like the vector stores we have been using. This type of strategy is called a \"hybrid search\". We will use BM25 and Chroma as our retrievers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "  \"I like apples\",\n",
    "  \"I like oranges\",\n",
    "  \"Apples and oranges are fruits\"\n",
    "]\n",
    "bm25 = BM25Retriever.from_texts(docs)\n",
    "bm25.k = 2\n",
    "chroma = Chroma.from_texts(docs, openai_model).as_retriever(search_kwargs = {\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EnsembleRetriever(\n",
    "  retrievers=[bm25, chroma],\n",
    "  weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples'),\n",
       " Document(page_content='Apples and oranges are fruits')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"apples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
